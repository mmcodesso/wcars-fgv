{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:03<00:00, 40.81it/s]\n"
     ]
    }
   ],
   "source": [
    "data['sentence_count'] = data['mda'].progress_apply(sentence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:12<00:00,  8.29it/s]\n"
     ]
    }
   ],
   "source": [
    "data['word_count'] = data['mda'].progress_apply(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syllable Count\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#This variable d will store the dictionary CMU\n",
    "d = cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_words(text):\n",
    "    \n",
    "    word_tokenize_list = word_tokenize(text)\n",
    "    complexwords = 0\n",
    "    \n",
    "    #This funcition return the count of syllables from a word that exists in the dictionary\n",
    "    def nsyl(word):\n",
    "        return int([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0])\n",
    "    \n",
    "    for word in word_tokenize_list:\n",
    "        syllables = 0\n",
    "        \n",
    "        if word.lower() in d:\n",
    "            syllables = nsyl(word)\n",
    "            \n",
    "        #Count only word with more than 2 syllables\n",
    "        if syllables >= 3:\n",
    "            complexwords += 1\n",
    "            \n",
    "    return complexwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:18<00:00,  4.91it/s]\n"
     ]
    }
   ],
   "source": [
    "data['complex_words'] = data['mda'].progress_apply(complex_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FOG_index'] = 0.4 * ((data['word_count']/data['sentence_count'])\n",
    "                           + 100 * (data['complex_words'] / data['word_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>company_name</th>\n",
       "      <th>date</th>\n",
       "      <th>industry_title</th>\n",
       "      <th>mda</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>complex_words</th>\n",
       "      <th>FOG_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1101215</td>\n",
       "      <td>ALLIANCE DATA SYSTEMS CORP</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSI\\n...</td>\n",
       "      <td>496</td>\n",
       "      <td>15360</td>\n",
       "      <td>2917</td>\n",
       "      <td>19.983451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>1057352</td>\n",
       "      <td>COSTAR GROUP INC</td>\n",
       "      <td>2017-02-24</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...</td>\n",
       "      <td>387</td>\n",
       "      <td>13283</td>\n",
       "      <td>2735</td>\n",
       "      <td>21.965290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>1077561</td>\n",
       "      <td>China Elite Information Co., Ltd.</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...</td>\n",
       "      <td>24</td>\n",
       "      <td>751</td>\n",
       "      <td>188</td>\n",
       "      <td>22.529982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>1112985</td>\n",
       "      <td>Delta International Oil &amp; Gas Inc.</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS O...</td>\n",
       "      <td>108</td>\n",
       "      <td>3306</td>\n",
       "      <td>674</td>\n",
       "      <td>20.399314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>1065088</td>\n",
       "      <td>EBAY INC</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...</td>\n",
       "      <td>356</td>\n",
       "      <td>13101</td>\n",
       "      <td>2538</td>\n",
       "      <td>22.469252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik                        company_name       date  \\\n",
       "107   1101215          ALLIANCE DATA SYSTEMS CORP 2017-02-27   \n",
       "1094  1057352                    COSTAR GROUP INC 2017-02-24   \n",
       "1283  1077561   China Elite Information Co., Ltd. 2017-02-28   \n",
       "1510  1112985  Delta International Oil & Gas Inc. 2017-03-31   \n",
       "1568  1065088                            EBAY INC 2017-02-06   \n",
       "\n",
       "                       industry_title  \\\n",
       "107   SERVICES-BUSINESS SERVICES, NEC   \n",
       "1094  SERVICES-BUSINESS SERVICES, NEC   \n",
       "1283  SERVICES-BUSINESS SERVICES, NEC   \n",
       "1510  SERVICES-BUSINESS SERVICES, NEC   \n",
       "1568  SERVICES-BUSINESS SERVICES, NEC   \n",
       "\n",
       "                                                    mda  sentence_count  \\\n",
       "107   ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSI\\n...             496   \n",
       "1094  ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...             387   \n",
       "1283  ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...              24   \n",
       "1510  ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS O...             108   \n",
       "1568  ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...             356   \n",
       "\n",
       "      word_count  complex_words  FOG_index  \n",
       "107        15360           2917  19.983451  \n",
       "1094       13283           2735  21.965290  \n",
       "1283         751            188  22.529982  \n",
       "1510        3306            674  20.399314  \n",
       "1568       13101           2538  22.469252  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json('./data/mda.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = pd.read_fwf('./LoughranMcDonald_Negative.txt',header=None, \n",
    "                       names=['negatives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABANDON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDONED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABANDONMENTS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      negatives\n",
       "0       ABANDON\n",
       "1     ABANDONED\n",
       "2    ABANDONING\n",
       "3   ABANDONMENT\n",
       "4  ABANDONMENTS"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stemmed = [stemmer.stem(word) for word in neg_words['negatives']]\n",
    "neg_words['stemm'] = neg_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatives</th>\n",
       "      <th>stemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABANDON</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABANDONED</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABANDONING</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABANDONMENT</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABANDONMENTS</td>\n",
       "      <td>abandon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      negatives    stemm\n",
       "0       ABANDON  abandon\n",
       "1     ABANDONED  abandon\n",
       "2    ABANDONING  abandon\n",
       "3   ABANDONMENT  abandon\n",
       "4  ABANDONMENTS  abandon"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word = set(list(neg_words['negatives']))\n",
    "neg_stemmed = set(list(neg_words['stemm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def negative_count (data, negative):    \n",
    "    return len([word for word in word_tokenize(data) if word in negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = []\n",
    "negative_stemmed_words = []\n",
    "\n",
    "for mda in data['mda']:\n",
    "    negative_words.append(negative_count(mda,negative=neg_word))\n",
    "    \n",
    "    stemmed_text = \" \".join([stemmer.stem(word) for word in word_tokenize(mda)])\n",
    "    negative_stemmed_words.append(negative_count(stemmed_text,negative=neg_stemmed))\n",
    "    \n",
    "data['negative_words'] = negative_words\n",
    "data['negative_stemmed_words'] = negative_stemmed_words      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOG_index</th>\n",
       "      <th>cik</th>\n",
       "      <th>company_name</th>\n",
       "      <th>complex_words</th>\n",
       "      <th>date</th>\n",
       "      <th>industry_title</th>\n",
       "      <th>mda</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>negative_words</th>\n",
       "      <th>negative_stemmed_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>19.983451</td>\n",
       "      <td>1101215</td>\n",
       "      <td>ALLIANCE DATA SYSTEMS CORP</td>\n",
       "      <td>2917</td>\n",
       "      <td>2017-02-27</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSI\\n...</td>\n",
       "      <td>496</td>\n",
       "      <td>15360</td>\n",
       "      <td>125</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>21.965290</td>\n",
       "      <td>1057352</td>\n",
       "      <td>COSTAR GROUP INC</td>\n",
       "      <td>2735</td>\n",
       "      <td>2017-02-24</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...</td>\n",
       "      <td>387</td>\n",
       "      <td>13283</td>\n",
       "      <td>130</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>22.529982</td>\n",
       "      <td>1077561</td>\n",
       "      <td>China Elite Information Co., Ltd.</td>\n",
       "      <td>188</td>\n",
       "      <td>2017-02-28</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...</td>\n",
       "      <td>24</td>\n",
       "      <td>751</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>20.399314</td>\n",
       "      <td>1112985</td>\n",
       "      <td>Delta International Oil &amp; Gas Inc.</td>\n",
       "      <td>674</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS O...</td>\n",
       "      <td>108</td>\n",
       "      <td>3306</td>\n",
       "      <td>40</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>22.469252</td>\n",
       "      <td>1065088</td>\n",
       "      <td>EBAY INC</td>\n",
       "      <td>2538</td>\n",
       "      <td>2017-02-06</td>\n",
       "      <td>SERVICES-BUSINESS SERVICES, NEC</td>\n",
       "      <td>ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...</td>\n",
       "      <td>356</td>\n",
       "      <td>13101</td>\n",
       "      <td>128</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FOG_index      cik                        company_name  complex_words  \\\n",
       "107   19.983451  1101215          ALLIANCE DATA SYSTEMS CORP           2917   \n",
       "1094  21.965290  1057352                    COSTAR GROUP INC           2735   \n",
       "1283  22.529982  1077561   China Elite Information Co., Ltd.            188   \n",
       "1510  20.399314  1112985  Delta International Oil & Gas Inc.            674   \n",
       "1568  22.469252  1065088                            EBAY INC           2538   \n",
       "\n",
       "           date                   industry_title  \\\n",
       "107  2017-02-27  SERVICES-BUSINESS SERVICES, NEC   \n",
       "1094 2017-02-24  SERVICES-BUSINESS SERVICES, NEC   \n",
       "1283 2017-02-28  SERVICES-BUSINESS SERVICES, NEC   \n",
       "1510 2017-03-31  SERVICES-BUSINESS SERVICES, NEC   \n",
       "1568 2017-02-06  SERVICES-BUSINESS SERVICES, NEC   \n",
       "\n",
       "                                                    mda  sentence_count  \\\n",
       "107   ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSI\\n...             496   \n",
       "1094  ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...             387   \n",
       "1283  ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...              24   \n",
       "1510  ITEM 7. MANAGEMENT’S DISCUSSION AND ANALYSIS O...             108   \n",
       "1568  ITEM 7.\\nMANAGEMENT’S DISCUSSION AND ANALYSIS ...             356   \n",
       "\n",
       "      word_count  negative_words  negative_stemmed_words  \n",
       "107        15360             125                     205  \n",
       "1094       13283             130                     209  \n",
       "1283         751               6                      14  \n",
       "1510        3306              40                      62  \n",
       "1568       13101             128                     203  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the results\n",
    "data.to_json('./data/mda.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above, the count of stemmed negative words often more than doubles the count of negative words. Does this indicate superior performance by the stemmed method? In the next lines of code we match the stemmed words to their original negative words. We find that the words content, subject, and liquid are a part of the stemmed negative word list. The stemmed negative word list has become too abstract and is now over estimating the number of negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_print (data, negative=neg_word):    \n",
    "    return [word for word in word_tokenize(data) if word in negative]\n",
    "\n",
    "negative_word_print = []\n",
    "negative_stemmed_print = []\n",
    "\n",
    "for mda in data['mda'].head(1):\n",
    "    negative_word_print.append(negative_print(mda,negative=neg_word))\n",
    "    \n",
    "    stemmed_text = \" \".join([stemmer.stem(word) for word in word_tokenize(mda)])\n",
    "    negative_stemmed_print.append(negative_print(stemmed_text,negative=neg_stemmed))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'limit', 'contract', 'declin', 'violat', 'subject', 'bankrupt', 'excess', 'critic', 'deterior', 'will', 'deceas', 'weak', 'liquid', 'unwil', 'bankruptci', 'breakag', 'delinqu', 'slow', 'concern', 'cancel', 'arrear', 'expos', 'defer', 'disclos', 'failur', 'late', 'loss', 'laps', 'correct', 'penalti', 'unabl', 'unpaid', 'uncollect', 'termin', 'close', 'downturn', 'deni', 'impair', 'bridg', 'content', 'neg', 'sever', 'object', 'fraud'}\n"
     ]
    }
   ],
   "source": [
    "print(set(negative_stemmed_print[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negatives</th>\n",
       "      <th>stemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>CONTENTION</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>CONTENTIONS</td>\n",
       "      <td>content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>SUBJECTED</td>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>SUBJECTING</td>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>SUBJECTION</td>\n",
       "      <td>subject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        negatives    stemm\n",
       "341    CONTENTION  content\n",
       "342   CONTENTIONS  content\n",
       "2013    SUBJECTED  subject\n",
       "2014   SUBJECTING  subject\n",
       "2015   SUBJECTION  subject"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words[(neg_words.stemm == 'content') | (neg_words.stemm == 'subject')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the results of the analysis above, or any dictionary-based or bag-of-words analysis, is to apply weights to the words based on their importance, or significance to the document in which they are found. Term frequency – inverse document frequency (TF-IDF) is a popular weighting algorithm for doing this. The idea behind TF-IDF is that terms thart are common in one document, but uncommon in the corpus, are important to describing the nature or topic of that one document. Terms that are common in every document, such as prepositions and determiners, are not as useful for describing the nature or topic of any document. Likewise, terms that appear infrequently in every document are also relatively unimportant. The formula for TF-IDF is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing TF-IDF in code is fairly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 95)\t0.00204082148842\n",
      "  (0, 42)\t0.00208370196469\n",
      "  (0, 71)\t0.00217878056615\n",
      "  (0, 32)\t0.00223196228408\n",
      "  (0, 14)\t0.00208370196469\n",
      "  (0, 93)\t0.00200054147252\n",
      "  (0, 89)\t0.00192664129441\n",
      "  (0, 1)\t0.00237241983961\n",
      "  (0, 67)\t0.00171824194977\n",
      "  (0, 46)\t0.00204082148842\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = data['mda']\n",
    "corpus_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "print(corpus_tfidf[0,0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common task in text analytics is to measure document similarity, and one measure is Cosine Similarity. Cosine similarity takes as an input two same-length vectors and returns the cosine of the angle between those two vectors. If the angle is 0 degrees, then the vectors are identical and the cosine value is 1. If the angle is 90 degrees, then the vectors receive a cosine similarity score of zero.\n",
    "\n",
    "In text analytics, vectors are constructed based on the count of words in two documents. Imagine we are evaluating the similarity of S1, S2 and S3:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;S1: I read a book for a book club.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;S2: I read a magazine.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;S3: You read a pamphlet for a book club.\n",
    "\n",
    "\n",
    "The vectors for these three strings can be constructed as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||a|book|club|for|i|magazine|pamphlet|read|you|\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**S1**|2|2|1|1|1|0|i|1|0|\n",
    "|**S2**|1|0|0|0|1|1|i|1|0|\n",
    "|**S3**|2|1|1|1|0|0|i|1|1|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine between two strings is: s = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\overline{x})^2}{N-1} }, or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the performance of this algorithm, for each string we can replace the raw counts of each word with the tf-idf weighted score. Tf-idf has two components, the frequency of each term, and the inverse document frequency of each term. The following table show the tf score for each word and for each document. The numerator is the number of times the word appears in the sentence. The denominator is the number of words in the sentence. Looking at the term frequency score, it is difficult to tell which word is the most important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||a|book|club|for|i|magazine|pamphlet|read|you|\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**S1**|2/8|2/8|1/8|1/8|1/8|0|0|1/8|0|\n",
    "|**S2**|1/4|0|0|0|1/4|1/4|0|1/4|0|\n",
    "|**S3**|2/8|1/8|1/8|1/8|0|0|1/8|1/8|1/8|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse document frequency is a constant that applies to each word over the entire corpus. The idf(“a”) is the log of 3/3 because the word “a” appears in three out of three documents while idf(“book”) is log(3/1) because book appears in only one document. The next table shows the idf values for each word. Since idf is constant for each word across all documents in a corpus, the idf value does not change by document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||a|book|club|for|i|magazine|pamphlet|read|you|\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**Sn**|log(3/3)|log(3/2)|log(3/2)|log(3/2)|log(3/2)|log(3/1)|log(3/1)|log(3/3)|log(3/1)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the tf-idf weighted value of each word in each document, we now multiply the corresponding cells together and get the following values (rounded to two decimal places). For the first cell, the resulting value is 0 because log(3/3) = log(1) = 0. Thus, “a” is not an important word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||a|book|club|for|i|magazine|pamphlet|read|you|\n",
    "|:---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "|**S1**|0|0.044|0.022|.022|.022|0|0|0|0|\n",
    "|**S2**|0|0|0|0|.044|0.119|0|0|0|\n",
    "|**S3**|0|0.022|0.022|.022|0|0|0.060|0|0.060|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the results of the Cosine Similarity algorithm comparing S1, S2, and S3. The top-right cells are the comparisons before the terms are weighted with tf-idf. The bottom left cells are the comparisons after the tf-idf weightings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||S1|S2|S3|\n",
    "|---|---|---|---|\n",
    "|S1||0.577|0.822|\n",
    "|S2|0.131||0.474|\n",
    "|S3|0.358|0.000||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = data['mda']\n",
    "corpus_count = count_vectorizer.fit_transform(corpus)\n",
    "corpus_tfidf = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_count = cosine_similarity(corpus_count)\n",
    "cosine_tfidf = cosine_similarity(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      0.765   0.6086 ...,  0.5167  0.7776  0.736 ]\n",
      " [ 0.765   1.      0.6876 ...,  0.5487  0.8912  0.8495]\n",
      " [ 0.6086  0.6876  1.     ...,  0.5034  0.6924  0.6438]\n",
      " ..., \n",
      " [ 0.5167  0.5487  0.5034 ...,  1.      0.5715  0.5289]\n",
      " [ 0.7776  0.8912  0.6924 ...,  0.5715  1.      0.8725]\n",
      " [ 0.736   0.8495  0.6438 ...,  0.5289  0.8725  1.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(cosine_tfidf,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      0.9177  0.767  ...,  0.8608  0.9214  0.8846]\n",
      " [ 0.9177  1.      0.7858 ...,  0.8388  0.9626  0.9363]\n",
      " [ 0.767   0.7858  1.     ...,  0.789   0.774   0.7366]\n",
      " ..., \n",
      " [ 0.8608  0.8388  0.789  ...,  1.      0.8525  0.8012]\n",
      " [ 0.9214  0.9626  0.774  ...,  0.8525  1.      0.9408]\n",
      " [ 0.8846  0.9363  0.7366 ...,  0.8012  0.9408  1.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(cosine_count,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: 3 tf-idf: 7 count: 89\n",
      "Document: 4 tf-idf: 28 count: 119\n",
      "Document: 5 tf-idf: 118 count: 66\n",
      "Document: 6 tf-idf: 117 count: 24\n",
      "Document: 8 tf-idf: 42 count: 48\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    tfidf = np.argmax(cosine_tfidf[i,i+1:])\n",
    "    count = np.argmax(cosine_count[i,i+1:])\n",
    "    if tfidf != count:\n",
    "        print('Document:', i, 'tf-idf:', tfidf, 'count:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes Classification Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naïve Bayes classification algorithm is widely used as a baseline algorithm for machine learning performance. It is simple, intuitive, and easy to implement. Documents can be classified in any number of ways, but to evaluate performance of a classification algorithm, the outcome variable must be known beforehand. In this example, we are going to classify MD&A text according to industry. In other words, our classifier will determine which industry an MD&A comes from, and we will evaluate the results based on the actual industry association. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['industry_title']\n",
    "x_train_count, x_test_count, y_train_count, y_test_count = train_test_split(corpus_count, y, random_state=42)\n",
    "x_train_tfidf, x_test_tfidf, y_train_ifidf, y_test_tfidf = train_test_split(corpus_count, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classifier = MultinomialNB()\n",
    "tfidf_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classifier.fit(x_train_count,y_train_count)\n",
    "tfidf_classifier.fit(x_train_tfidf,y_train_ifidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_count = count_classifier.predict(x_test_count)\n",
    "y_predict_tfidf = tfidf_classifier.predict(x_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "SERVICES-BUSINESS SERVICES, NEC       0.77      0.53      0.62        19\n",
      "  SERVICES-PREPACKAGED SOFTWARE       0.53      0.77      0.62        13\n",
      "\n",
      "                    avg / total       0.67      0.62      0.62        32\n",
      "\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "SERVICES-BUSINESS SERVICES, NEC       0.77      0.53      0.62        19\n",
      "  SERVICES-PREPACKAGED SOFTWARE       0.53      0.77      0.62        13\n",
      "\n",
      "                    avg / total       0.67      0.62      0.62        32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test_count, y_predict_count))\n",
    "\n",
    "print(metrics.classification_report(y_test_tfidf, y_predict_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10  9]\n",
      " [ 3 10]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test_count, y_predict_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
